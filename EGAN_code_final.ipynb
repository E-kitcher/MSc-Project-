{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EGAN_code_final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gQCeb45AG5dn",
        "outputId": "b802dab6-daa3-4bf3-f4af-440b3d69fda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "import copy\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from datetime import datetime\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vArsqD5XG5ds",
        "colab": {}
      },
      "source": [
        "# Data params\n",
        "\n",
        "# ##### DATA: Target data and generator input data\n",
        "def get_x(size, num_of_gaus=None):\n",
        "  if num_of_gaus ==8:\n",
        "    scale = 2.\n",
        "    centers = [\n",
        "        (1,0),\n",
        "        (-1,0),\n",
        "        (0,1),\n",
        "        (0,-1),\n",
        "        (1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "        (1./np.sqrt(2), -1./np.sqrt(2)),\n",
        "        (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "        (-1./np.sqrt(2), -1./np.sqrt(2))\n",
        "    ]\n",
        "    centers = [(scale*x,scale*y) for x,y in centers]\n",
        "    dataset = []\n",
        "    for i in range(size):\n",
        "        point = np.random.randn(2)*.02\n",
        "        center = random.choice(centers)\n",
        "        point[0] += center[0]\n",
        "        point[1] += center[1]\n",
        "        dataset.append(point)\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "#     dataset /= 1.414 # stdev\n",
        "    return torch.from_numpy(dataset )\n",
        "\n",
        "  if num_of_gaus == 25:\n",
        "    centers = []\n",
        "    for x in range(-2,3):\n",
        "      for y in range(-2,3):\n",
        "        centers.append((x,y))  \n",
        "      dataset = []\n",
        "    for i in range(size): # consider doing more points\n",
        "        point = np.random.randn(2)*.02\n",
        "        center = random.choice(centers)\n",
        "        point[0] += center[0]\n",
        "        point[1] += center[1]\n",
        "        dataset.append(point)\n",
        "    dataset = np.array(dataset, dtype='float32')\n",
        "#     dataset /= 2.828 # stdev\n",
        "    return torch.from_numpy(dataset)\n",
        "\n",
        "  if num_of_gaus == 0:\n",
        "    data = datasets.make_swiss_roll(\n",
        "        n_samples=size, \n",
        "        noise=0.25\n",
        "    )[0]\n",
        "    dataset = data.astype('float32')[:, [0, 2]]\n",
        "    dataset /= 7.5 # stdev plus a little\n",
        "    return torch.from_numpy(dataset)\n",
        "\n",
        "\n",
        "def get_z(m, n=2):  # sample_Z(batch_size,2) \n",
        "    return torch.tensor(np.random.uniform(-1., 1., size=[m, n]), dtype=torch.float32 )\n",
        "\n",
        "\n",
        "         \n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "  \n",
        "def calc_g_loss(loss_type = None, d_gz = None , mini_batch_size= None, loss_BCE = None ,loss_MSE= None ):\n",
        "  if loss_type == 'logD': #  - w * ( ylog(D(x)) + (1-y)log(1 - D(x)) )  \n",
        "    g_loss = loss_BCE(d_gz, Variable(torch.ones([mini_batch_size,1])).to(DEVICE)) # - E[ log(D(G(Z)) ]\n",
        "      \n",
        "  elif loss_type =='minimax':\n",
        "    g_loss = (-1)*loss_BCE(d_gz, Variable(torch.zeros([mini_batch_size,1])).to(DEVICE)) # E[ log( 1 - D(G(Z)) ) ]\n",
        "        \n",
        "  elif loss_type=='ls':\n",
        "    g_loss = loss_MSE(d_gz, Variable(torch.ones([mini_batch_size,1])).to(DEVICE)) #  mean ( (x - y)**2 ) -> E[D(G(Z)) - 1)^2]\n",
        "    \n",
        "  return g_loss\n",
        "  \n",
        "def get_fit(some_list):\n",
        "      return np.argmax(some_list)\n",
        "  \n",
        "def calc_fq(d_fake_decision = None):\n",
        "    return torch.mean(d_fake_decision).data.cpu().numpy() # old torch.mean(d_fake_error).detach().data.cpu().numpy()\n",
        "  \n",
        "def calc_fd(d_real_decision, d_fake_error, loss_BCE, D, mini_batch_size=None):\n",
        "    V = loss_BCE(d_real_decision, Variable(torch.ones([mini_batch_size,1])).to(DEVICE)) \\\n",
        "    + d_fake_error\n",
        "\n",
        "    delta_D = torch.autograd.grad(outputs=V, inputs= D.parameters(),\n",
        "                          grad_outputs=torch.ones(V.size()).to(DEVICE),\n",
        "                          only_inputs=True)\n",
        "    with torch.no_grad():\n",
        "        for i, grad in enumerate(delta_D):\n",
        "            grad = grad.view(-1)\n",
        "            allgrad = grad if i == 0 else torch.cat([allgrad,grad]) \n",
        "        Fd = torch.log(torch.norm(allgrad)).detach().data.cpu().numpy() # usewd to be .numpy\n",
        "    return Fd\n",
        "\n",
        "def calc_precision(d_real_decision):\n",
        "    return  (torch.sum(d_real_decision >= 0.5, dtype=torch.float)/ len(d_real_decision)).item() # rule will return 1 or 0, sum over these and divide by total to get precision (% classed correctly)\n",
        "    \n",
        "\n",
        "def calc_F(fq,fd,gamma1,gamma2):\n",
        "    return (gamma1*fq + gamma2*fd)\n",
        "\n",
        "\n",
        "def get_D_decision_from_fake(g_sample = None, G= None, D= None): #DETACHES\n",
        "    d_gen_input = Variable(g_sample).to(DEVICE) #gi_sampler(minibatch_size, g_input_size)\n",
        "    d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "    d_fake_decision = D(d_fake_data)\n",
        "    return d_fake_decision\n",
        "\n",
        "def get_D_decision_from_real(d_sampler = None, G= None, D= None):\n",
        "    d_real_data = Variable(d_sampler).to(DEVICE) \n",
        "    d_real_decision = D(d_real_data) \n",
        "    return d_real_decision\n",
        "    \n",
        "    \n",
        "\n",
        "def create_save_figure():\n",
        "    plt.figure()\n",
        "    g_plot = g_fake_data.detach().cpu().numpy()\n",
        "    xax = plt.scatter(x_plot[:,0], x_plot[:,1], s=10)\n",
        "    gax = plt.scatter(g_plot[:,0],g_plot[:,1], s=1)\n",
        "\n",
        "    plt.legend((xax,gax), (\"Real Data\",\"Generated Data\"))\n",
        "    plt.savefig('final_plot_'+dt+'.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "def result_evaluate(dataset, num_of_gaus, radi=0.1):\n",
        "# for each node, calculate the number of points within a radius\n",
        "# each node should have n/num_of_gaus, allow each node a 5% variation, anything over/under that reduce its score\n",
        "# so if the node has 80 when it should have 100 score it 80/100, likewise for 130 -> 70/100\n",
        "# sum up the scores and take an average\n",
        "    size = len(dataset)\n",
        "    centers = centers_test(num_of_gaus)\n",
        "    dataset = dataset\n",
        "    i=0\n",
        "    scores = [0] * len(centers)\n",
        "    for center in centers:\n",
        "        x,y = center\n",
        "        for data in dataset:\n",
        "            if x-radi <= data[0] <= x+radi and y-radi <= data[1] <= y+radi:\n",
        "                scores[i] += 1\n",
        "        i+=1\n",
        "    scores=np.array(scores, dtype='float32')\n",
        "#     print(scores)\n",
        "#         print(sum(scores))\n",
        "    scores/=(size/num_of_gaus)\n",
        "    # scores are now decimals\n",
        "\n",
        "    for i in range(len(scores)):\n",
        "        if scores[i] > 2:\n",
        "          scores[i] = 0.1\n",
        "        if scores[i] > 1:\n",
        "          scores[i] = abs(1 - round(scores[i] -1, 3) )\n",
        "#         print(scores)   \n",
        "    total_score = sum(scores)/num_of_gaus\n",
        "#     print(total_score)\n",
        "    return total_score\n",
        "\n",
        "def centers_test(num_of_centers):\n",
        "  if num_of_centers==8:\n",
        "    scale = 2.\n",
        "    centers = [\n",
        "        (1,0),\n",
        "        (-1,0),\n",
        "        (0,1),\n",
        "        (0,-1),\n",
        "        (1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "        (1./np.sqrt(2), -1./np.sqrt(2)),\n",
        "        (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
        "        (-1./np.sqrt(2), -1./np.sqrt(2))\n",
        "    ]\n",
        "    centers = [(scale*x,scale*y) for x,y in centers]\n",
        "    a=[]\n",
        "    for center in centers:\n",
        "        b=[0,0]\n",
        "        b[0] = center[0]\n",
        "        b[1] = center[1]\n",
        "        a+=[b]\n",
        "    centers = np.array(a)\n",
        "    return centers\n",
        "\n",
        "  if num_of_centers == 25:\n",
        "    centers = []\n",
        "    for x in range(-2,3):\n",
        "      for y in range(-2,3):\n",
        "        centers.append((x,y)) \n",
        "    return centers\n",
        "  \n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size =2 , hidden_size = 512, output_size = 2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.map2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.map3(x)\n",
        "        return x\n",
        "    def init_normal_weights(self):\n",
        "        var = 0.02\n",
        "        self.map1.weight.data.normal_(0.0,var)\n",
        "        self.map1.bias.data.normal_(0.0, var)\n",
        "        self.map2.weight.data.normal_(0.0, var)\n",
        "        self.map2.bias.data.normal_(0.0, var)\n",
        "        self.map3.weight.data.normal_(0.0, var)\n",
        "        self.map3.bias.data.normal_(0.0, var)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size = 2, hidden_size=512, output_size=1): # f =torch.sigmoid\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.map1(x))\n",
        "        x = F.leaky_relu(self.map2(x))\n",
        "        x = F.leaky_relu(self.map3(x))\n",
        "        return F.sigmoid(self.out(x))\n",
        "    def init_normal_weights(self):\n",
        "        var = 0.02\n",
        "        self.map1.weight.data.normal_(0.0,var)\n",
        "        self.map1.bias.data.normal_(0.0, var)\n",
        "        self.map2.weight.data.normal_(0.0, var)\n",
        "        self.map2.bias.data.normal_(0.0, var)\n",
        "        self.map3.weight.data.normal_(0.0, var)\n",
        "        self.map3.bias.data.normal_(0.0, var)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY2l4LIXVqgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_results_version(mutation_types =  ['logD', 'minimax', 'ls'], \n",
        "        d_learning_rate = 0.0001,g_learning_rate = 0.0001,\n",
        "        d_steps = 1, g_steps=1,\n",
        "        save_logs = True, save_figure = True, print_fig = True,\n",
        "        num_of_gaus = 8, g1= 1, g2=1, random_seed_val = 42\n",
        "          ):\n",
        "#     manualSeed = 42\n",
        "#     random.seed(manualSeed)\n",
        "#     torch.manual_seed(manualSeed)   \n",
        "    dt = str( datetime.today().strftime('%Y-%m-%d-%H:%M') ) \n",
        "    dt = dt.replace(':', '-')\n",
        "    \n",
        "    # Model parameters  \n",
        "    minibatch_size = 64\n",
        "    batchSize = 256 #they use 64 for small things but 256 for the images \n",
        "    num_epochs = 50 * 1000\n",
        "    show_epoch =  num_epochs  *0.1    \n",
        "    x_plot = get_x(batchSize,num_of_gaus)\n",
        "    \n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    d_sampler = get_x(minibatch_size,num_of_gaus)\n",
        "    gi_sampler = get_z(minibatch_size)\n",
        "    G = Generator().to(DEVICE)\n",
        "    D = Discriminator().to(DEVICE)\n",
        "    G.init_normal_weights()\n",
        "    D.init_normal_weights()\n",
        "    \n",
        "    loss_BCE = nn.BCELoss().to(DEVICE) #Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "#     loss_BCE = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "    loss_MSE = nn.MSELoss().to(DEVICE)\n",
        "    d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=[0.5, 0.99])\n",
        "    g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=[0.5, 0.99])\n",
        "    \n",
        "#     mutation_types =  ['logD', 'minimax', 'ls']\n",
        "    if save_logs==True:\n",
        "        fit_log = np.zeros(shape=(num_epochs,len(mutation_types)*2))\n",
        "        \n",
        "    tally = [0,0,0]\n",
        "    mut_freq = []\n",
        "    green_plot = []\n",
        "    \n",
        "    for epoch in range(num_epochs):#num_epochs\n",
        "        for d_index in range(d_steps):\n",
        "            # Get data\n",
        "            D.zero_grad()\n",
        "            d_sampler = get_x(minibatch_size,num_of_gaus)\n",
        "            gi_sampler = get_z(minibatch_size)\n",
        "            #  1A: Train D on real \n",
        "             \n",
        "            d_real_decision = get_D_decision_from_real(d_sampler, G, D)\n",
        "            d_real_error = loss_BCE(d_real_decision, Variable(torch.ones([minibatch_size,1])).to(DEVICE))  # ones = true (y=1), x=D(x) ... -( ylog(D(x)) + (1-y)log(1 - D(x)) ) \n",
        "            d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "            #  1B: Train D on fake\n",
        "            d_fake_decision = get_D_decision_from_fake(gi_sampler, G, D) # DETACHES\n",
        "            d_fake_error = loss_BCE(d_fake_decision, Variable(torch.zeros([minibatch_size,1])).to(DEVICE))  # zeros = fake (y=0), x=D(g(z)) ... -( ylog(D(x)) + (1-y)log(1 - D(x)) ) \n",
        "            d_fake_error.backward()\n",
        "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0] # created fn returns errors\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "\n",
        "          \n",
        "            fitness_list = []\n",
        "            generator_list = []\n",
        "            counter = 0\n",
        "            for mutation in mutation_types:\n",
        "                G.zero_grad() # resets graph thingy\n",
        "                ''' get child from G's + get data  '''\n",
        "                child_G = copy.deepcopy(G)\n",
        "                \n",
        "                g_data = get_z(minibatch_size)\n",
        "                d_data = get_x(minibatch_size, num_of_gaus)\n",
        "                \n",
        "                # CALCULATE D OUTPUT TO UPDATE G NETWORK\n",
        "                d_gen_input = Variable(g_data).to(DEVICE) #gi_sampler(minibatch_size, g_input_size)\n",
        "                d_fake_data = child_G(d_gen_input)  # dont detach - want to train G here\n",
        "                d_fake_decision = D(d_fake_data)\n",
        "                ''' update network '''\n",
        "                # CALCULATE G LOSS\n",
        "                if mutation == 'logD': #  - w * ( ylog(D(x)) + (1-y)log(1 - D(x)) )  \n",
        "                  child_loss =      loss_BCE(d_fake_decision, Variable(torch.ones([minibatch_size,1])).to(DEVICE)) # - E[ log(D(G(Z)) ]\n",
        "                elif mutation =='minimax':\n",
        "                  child_loss = (-1)*loss_BCE(d_fake_decision, Variable(torch.zeros([minibatch_size,1])).to(DEVICE)) # E[ log( 1 - D(G(Z)) ) ]\n",
        "                elif mutation=='ls':\n",
        "                  child_loss =      loss_MSE(d_fake_decision, Variable(torch.ones([minibatch_size,1])).to(DEVICE)) #  mean ( (x - y)**2 ) -> E[D(G(Z)) - 1)^2]\n",
        "                  \n",
        "#                 child_loss = calc_g_loss(mutation, d_fake_decision, minibatch_size, loss_BCE, loss_MSE)\n",
        "                \n",
        "                child_loss.backward() # compute/store gradients, but don't change params\n",
        "    \n",
        "                child_opt = optim.Adam(child_G.parameters(), lr=g_learning_rate, betas=[0.5, 0.999])\n",
        "                child_opt.step()\n",
        "\n",
        "                ''' fitness '''\n",
        "                # Fq = Ez[D(G(z))] , Fd =  ( − log||∇D − Ex[log D(x)] − Ez[log(1 − D(G(z)))] || )             \n",
        "                d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "                d_fake_decision = D(d_fake_data)\n",
        "#                 d_fake_decision = get_D_decision_from_fake(g_data, child_G, D) # DETACHES\n",
        "\n",
        "                d_fake_error = loss_BCE(d_fake_decision, Variable(torch.zeros([minibatch_size,1])).to(DEVICE))  # zeros = fake\n",
        "                \n",
        "                Fq = calc_fq(d_fake_decision) # d_fake_decision = D(G(Z))\n",
        "                \n",
        "                d_real_data = Variable(d_data).to(DEVICE) \n",
        "                d_real_decision = D(d_real_data).detach() # dont use function as we need DETACH!\n",
        "                \n",
        "                Fd = calc_fd(d_real_decision, d_fake_error, loss_BCE, D, minibatch_size)\n",
        "#                 print('fq:',Fq)\n",
        "#                 print('fd:',Fd)\n",
        "                fit_log[epoch][counter] = float(Fq) # store the values in the log array\n",
        "                fit_log[epoch][counter+1] =  float(Fd)\n",
        "                \n",
        "                child_fitness = calc_F(Fq, Fd, gamma1= g1, gamma2= g2)\n",
        "                               \n",
        "                fitness_list += [child_fitness]\n",
        "                generator_list += [child_G] # putting the generator in a list ... is this a good idea?\n",
        "                counter += 2 # 2 so that it moves 2 columns over\n",
        "            ''' sort their fitnesses '''\n",
        "            rank = get_fit(fitness_list)\n",
        "            tally[rank] += 1\n",
        "            mut_freq += [rank]\n",
        "            G = generator_list[rank] \n",
        "            \n",
        "        ''' printing data '''\n",
        "            \n",
        "\n",
        "        g_data = get_z(batchSize) # batchSize\n",
        "        gen_input = Variable(g_data).to(DEVICE)\n",
        "        g_fake_data = G(gen_input).detach()\n",
        "\n",
        "        # uncomment this when rdy\n",
        "        if epoch % show_epoch == 0 and print_fig == True: # consider bringing back the extract fn\n",
        "            d_err = dre + dfe\n",
        "            print(\"Epoch %s: D (%s d_err) G (%s err); \" %\n",
        "                  (epoch, d_err, ge))\n",
        "            \n",
        "#             print('fitness', fitness_list)\n",
        "#             print(rank, 'rank')\n",
        "#             print(tally)\n",
        "          \n",
        "            plt.figure()\n",
        "            g_plot = g_fake_data.detach().cpu().numpy()\n",
        "#             green_plot.append(np.vstack([g_plot ] )) \n",
        "            xax = plt.scatter(x_plot[:,0], x_plot[:,1], s=10)\n",
        "            gax = plt.scatter(g_plot[:,0],g_plot[:,1], s=1)\n",
        "            print(len(g_plot))\n",
        "\n",
        "            plt.legend((xax,gax), (\"Real Data\",\"Generated Data\"))\n",
        "#             plt.savefig('final_plot_'+dt+'.png', bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "              \n",
        "    if save_figure == True:\n",
        "        create_save_figure\n",
        "    \n",
        "    if save_logs == True:\n",
        "        filename = 'fitlogs_'+dt+'.txt'    \n",
        "        np.savetxt(filename, fit_log)\n",
        "\n",
        "        mut_freq = np.array(mut_freq)\n",
        "        filename = 'mut_freq_logs'+dt+'.txt'\n",
        "        np.savetxt(filename, mut_freq)\n",
        "        \n",
        "        \n",
        "    g_data = get_z(1000) # batchSize\n",
        "    gen_input = Variable(g_data).to(DEVICE)\n",
        "    g_fake_data = G(gen_input).detach()\n",
        "    plt.figure()\n",
        "    g_plot = g_fake_data.detach().cpu().numpy()\n",
        "#             green_plot.append(np.vstack([g_plot ] )) \n",
        "    xax = plt.scatter(x_plot[:,0], x_plot[:,1], s=10)\n",
        "    gax = plt.scatter(g_plot[:,0],g_plot[:,1], s=1)\n",
        "    print(len(g_plot))\n",
        "\n",
        "    plt.legend((xax,gax), (\"Real Data\",\"Generated Data\"))\n",
        "#             plt.savefig('final_plot_'+dt+'.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    eval_score = result_evaluate(g_plot, num_of_gaus)\n",
        "    print(eval_score)\n",
        "           \n",
        "\n",
        "    g_plot = g_plot[:minibatch_size] #  g_fake_data.detach().cpu().numpy()    \n",
        "    limmax = 3\n",
        "    bg_color  = sns.color_palette('Greens', n_colors=256)[0]\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.xlim(-limmax, limmax)\n",
        "    plt.ylim(-limmax, limmax)\n",
        "    ax2 = sns.kdeplot(g_plot[:, 0], g_plot[:, 1], shade=True, cmap='Greens', n_levels=20, clip=[[-limmax,limmax]]*2)\n",
        "    ax2.set_facecolor(bg_color)  # set_axis_bgcolor(bg_color)\n",
        "    plt.xticks([]); plt.yticks([])\n",
        "#     plt.title('type %d'%(epoch+1))\n",
        "    ax2.set_ylabel('%d iteration:'% epoch )\n",
        "\n",
        "    \n",
        "    print(\"Finished!\")\n",
        "    return eval_score\n",
        "\n",
        "train_results_version() #mutation_types =  ['logD']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQlbi2pS5F2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}